{% set version = "0.0.3" %}
{% set posix = 'm2-' if win else '' %}
{% set native = 'm2w64-' if win else '' %}

package:
  name: r-tidyposterior
  version: {{ version|replace("-", "_") }}

source:
  url:
    - {{ cran_mirror }}/src/contrib/tidyposterior_{{ version }}.tar.gz
    - {{ cran_mirror }}/src/contrib/Archive/tidyposterior/tidyposterior_{{ version }}.tar.gz
  sha256: 0ff24ae844f2ac32f801bd655f2c439112df2ab3872a6e178edf14c001405b73

build:
  merge_build_host: true  # [win]
  number: 0
  noarch: generic
  rpaths:
    - lib/R/lib/
    - lib/

requirements:
  build:

  host:
    - r-base
    - r-dplyr
    - r-generics
    - r-ggplot2
    - r-purrr
    - r-rlang
    - r-rsample >=0.0.2
    - r-rstanarm >=2.15.3
    - r-tibble
    - r-tidyr >=0.7.1
  run:
    - r-base
    - r-dplyr
    - r-generics
    - r-ggplot2
    - r-purrr
    - r-rlang
    - r-rsample >=0.0.2
    - r-rstanarm >=2.15.3
    - r-tibble
    - r-tidyr >=0.7.1

test:
  commands:
    - $R -e "library('tidyposterior')"           # [not win]
    - "\"%R%\" -e \"library('tidyposterior')\""  # [win]

about:
  home: https://tidymodels.github.io/tidyposterior
  license: GPL-2
  summary: "Bayesian analysis used here to answer the question: \"when looking at resampling results, are the differences between models 'real'?\" To answer this, a model can be created were the performance statistic is the resampling statistics (e.g. accuracy or RMSE). These values are explained by the model types. In doing\
    \ this, we can get parameter estimates for each model's affect on performance and make statistical (and practical) comparisons between models. The methods included here are similar to Benavoli et al (2017) <http://jmlr.org/papers/v18/16-305.html>."
  license_family: GPL2
  license_file: {{ environ["PREFIX"] }}/lib/R/share/licenses/GPL-2

extra:
  recipe-maintainers:
    - conda-forge/r

# Package: tidyposterior
# Version: 0.0.2
# Title: Bayesian Analysis to Compare Models using Resampling Statistics
# Description: Bayesian analysis used here to answer the question: "when looking at resampling results, are the differences between models 'real'?" To answer this, a model can be created were the performance statistic is the resampling statistics (e.g. accuracy or RMSE). These values are explained by the model types. In doing this, we can get parameter estimates for each model's affect on performance and make statistical (and practical) comparisons between models. The methods included here are similar to Benavoli et al (2017) <http://jmlr.org/papers/v18/16-305.html>.
# Authors@R: c( person("Max", "Kuhn", , "max@rstudio.com", c("aut", "cre")), person("RStudio", role = "cph"))
# License: GPL-2
# URL: https://tidymodels.github.io/tidyposterior
# BugReports: https://github.com/tidymodels/tidyposterior/issues
# VignetteBuilder: knitr
# Encoding: UTF-8
# LazyData: true
# ByteCompile: true
# RoxygenNote: 6.1.0.9000
# Depends: R (>= 2.10)
# Imports: rsample (>= 0.0.2), tidyr (>= 0.7.1), dplyr, rstanarm (>= 2.15.3), rlang, utils, purrr, tibble, generics, ggplot2,
# Suggests: knitr, testthat, covr
# NeedsCompilation: no
# Packaged: 2018-11-15 19:50:59 UTC; max
# Author: Max Kuhn [aut, cre], RStudio [cph]
# Maintainer: Max Kuhn <max@rstudio.com>
# Repository: CRAN
# Date/Publication: 2018-11-15 20:20:03 UTC
